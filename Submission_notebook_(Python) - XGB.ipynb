{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Submission_notebook_(Python) - XGB.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rWq4q0Gfp1Ha"},"source":["<div style=\"text-align: center\">\n","  <img alt=\"AIcrowd\" src=\"https://gitlab.aicrowd.com/jyotish/pricing-game-notebook-scripts/raw/master/pricing-game-banner.png\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"uIBNi6PiHFfD"},"source":["# How to use this notebook üìù\n","\n","1. **Copy the notebook**. This is a shared template and any edits you make here will not be saved. _You should copy it into your own drive folder._ For this, click the \"File\" menu (top-left), then \"Save a Copy in Drive\". You can edit your copy however you like.\n","2. **Link it to your AICrowd account**. In order to submit your code to AICrowd, you need to provide your account's API key (see [_\"Configure static variables\"_](#static-var) for details).\n","3. **Stick to the function definitions**. The submission to AICrowd will look for the pre-defined function names:\n","  - `fit_model`\n","  - `save_model`\n","  - `load_model`\n","  - `predict_expected_claim`\n","  - `predict_premium`\n","\n","    Anything else you write outside of these functions will not be part of the final submission (including constants and utility functions), so make sure everything is defined within them, except for:\n","4. **Define your preprocessing**. In addition to the functions above, anything in the cell labelled [_\"Define your data preprocessing\"_](#data-preprocessing) will also be imported into your final submission. "]},{"cell_type":"markdown","metadata":{"id":"uor1bk8ud9Yf"},"source":["# Your pricing model üïµÔ∏è\n","\n","In this notebook, you can play with the data, and define and train your pricing model. You can then directly submit it to the AICrowd, with some magic code at the end.\n","\n","### Baseline logistic regression üí™\n","You can also play with a baseline logistic regression model [implemented here](https://colab.research.google.com/drive/1iDgDgWUw9QzOkbTYjeyY3i3DGuCoghs3?usp=sharing). "]},{"cell_type":"markdown","metadata":{"id":"KOG9aspEPfLo"},"source":["# Setup the notebook üõ†"]},{"cell_type":"code","metadata":{"id":"Cc9aD_S9w_Qs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611011326211,"user_tz":300,"elapsed":8676,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"e1b1cb32-0977-4e36-a8ff-c52c019bbc8d"},"source":["!bash <(curl -sL https://gitlab.aicrowd.com/jyotish/pricing-game-notebook-scripts/raw/master/python/setup.sh)\n","from aicrowd_helpers import *"],"execution_count":4,"outputs":[{"output_type":"stream","text":["‚öôÔ∏è Installing AIcrowd utilities...\n","  Running command git clone -q https://gitlab.aicrowd.com/yoogottamk/aicrowd-cli /tmp/pip-req-build-4job9stw\n","‚úÖ Installed AIcrowd utilities\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oWAkvr2mPqhO"},"source":["# Configure static variables üìé\n","<a name=\"static-var\"></a>\n","\n","In order to submit using this notebook, you must visit this URL https://aicrowd.com/participants/me and copy your API key. \n","\n","Then you must set the value of `AICROWD_API_KEY` wuth the value."]},{"cell_type":"code","metadata":{"id":"3Z8nmleFd9Yf","executionInfo":{"status":"ok","timestamp":1611011326954,"user_tz":300,"elapsed":9409,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["import sklearn\n","\n","class Config:\n","  TRAINING_DATA_PATH = 'training.csv'\n","  MODEL_OUTPUT_PATH = 'model.pkl'\n","  AICROWD_API_KEY = 'eaab81e0ad4d64a6b0e7ec99a89205f6'  # You can get the key from https://aicrowd.com/participants/me\n","  ADDITIONAL_PACKAGES = [\n","    'numpy',  # you can define versions as well, numpy==0.19.2\n","    'pandas',\n","    'scikit-learn==' + sklearn.__version__,\n","    \"tqdm\",\n","    \"xgboost\",\n","    \"\"\n","  ]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iK8Ki2WUjVoX"},"source":["# Download dataset files üíæ"]},{"cell_type":"code","metadata":{"id":"cgKzpAV0jVFQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611011338926,"user_tz":300,"elapsed":21373,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"267044c5-72ad-4ace-a743-7b30c18319b3"},"source":["# Make sure to offically join the challenge and accept the challenge rules! Otherwise you will not be able to download the data\n","%download_aicrowd_dataset"],"execution_count":6,"outputs":[{"output_type":"stream","text":["üíæ Downloading dataset...\n","Verifying API Key...\n","API Key valid\n","Saved API Key successfully!\n","‚úÖ Downloaded dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5wrBpC0qd9Yg"},"source":["# Packages üóÉ\n","\n","<a name=\"packages\"></a>\n","\n","Import here all the packages you need to define your model. **You will need to include all of these packages in `Config.ADDITIONAL_PACKAGES` for your code to run properly once submitted.**"]},{"cell_type":"code","metadata":{"id":"4q4C50Fsd9Yg","executionInfo":{"status":"ok","timestamp":1611011338927,"user_tz":300,"elapsed":21365,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["%%track_imports\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from tqdm import tqdm\n","import itertools\n","import json\n","\n","import xgboost as xgb\n","from xgboost.sklearn import XGBClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_validate\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR62QOUGd9Yg","executionInfo":{"status":"ok","timestamp":1611011339073,"user_tz":300,"elapsed":21505,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["import importlib\n","import global_imports\n","importlib.reload(global_imports)\n","from global_imports import *  # do not change this"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRgsbwWwd9Yg"},"source":["# Loading the data üì≤"]},{"cell_type":"code","metadata":{"id":"vQQghMU7d9Yg","executionInfo":{"status":"ok","timestamp":1611011339874,"user_tz":300,"elapsed":22298,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["df = pd.read_csv(Config.TRAINING_DATA_PATH)\n","X_train = df.drop(columns=['claim_amount'])\n","y_train = df['claim_amount']"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WArx8uDQd9Yh"},"source":["## How does the data look like? üîç"]},{"cell_type":"code","metadata":{"id":"O_dyebPyQbSO","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1611011340133,"user_tz":300,"elapsed":22547,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"e93bc263-dac3-49a4-9cb5-83ae9ebbf5ad"},"source":["X_train.sample(n=4)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_policy</th>\n","      <th>year</th>\n","      <th>pol_no_claims_discount</th>\n","      <th>pol_coverage</th>\n","      <th>pol_duration</th>\n","      <th>pol_sit_duration</th>\n","      <th>pol_pay_freq</th>\n","      <th>pol_payd</th>\n","      <th>pol_usage</th>\n","      <th>drv_sex1</th>\n","      <th>drv_age1</th>\n","      <th>drv_age_lic1</th>\n","      <th>drv_drv2</th>\n","      <th>drv_sex2</th>\n","      <th>drv_age2</th>\n","      <th>drv_age_lic2</th>\n","      <th>vh_make_model</th>\n","      <th>vh_age</th>\n","      <th>vh_fuel</th>\n","      <th>vh_type</th>\n","      <th>vh_speed</th>\n","      <th>vh_value</th>\n","      <th>vh_weight</th>\n","      <th>population</th>\n","      <th>town_surface_area</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>184715</th>\n","      <td>PL076370</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>Max</td>\n","      <td>14</td>\n","      <td>8</td>\n","      <td>Yearly</td>\n","      <td>No</td>\n","      <td>WorkPrivate</td>\n","      <td>M</td>\n","      <td>60.0</td>\n","      <td>42.0</td>\n","      <td>Yes</td>\n","      <td>M</td>\n","      <td>30.0</td>\n","      <td>12.0</td>\n","      <td>rthsjeyjgdlmkygk</td>\n","      <td>8.0</td>\n","      <td>Diesel</td>\n","      <td>Tourism</td>\n","      <td>182.0</td>\n","      <td>12820.0</td>\n","      <td>1036.0</td>\n","      <td>440.0</td>\n","      <td>224.8</td>\n","    </tr>\n","    <tr>\n","      <th>80782</th>\n","      <td>PL064491</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>Med2</td>\n","      <td>23</td>\n","      <td>3</td>\n","      <td>Yearly</td>\n","      <td>No</td>\n","      <td>Retired</td>\n","      <td>M</td>\n","      <td>66.0</td>\n","      <td>48.0</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>ixwsqebjjdlxcqsq</td>\n","      <td>19.0</td>\n","      <td>Gasoline</td>\n","      <td>Tourism</td>\n","      <td>110.0</td>\n","      <td>10093.0</td>\n","      <td>0.0</td>\n","      <td>280.0</td>\n","      <td>368.2</td>\n","    </tr>\n","    <tr>\n","      <th>168786</th>\n","      <td>PL077629</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>Max</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>Yearly</td>\n","      <td>No</td>\n","      <td>WorkPrivate</td>\n","      <td>F</td>\n","      <td>48.0</td>\n","      <td>29.0</td>\n","      <td>Yes</td>\n","      <td>M</td>\n","      <td>56.0</td>\n","      <td>37.0</td>\n","      <td>quslbttvcitxzeiy</td>\n","      <td>4.0</td>\n","      <td>Diesel</td>\n","      <td>Tourism</td>\n","      <td>200.0</td>\n","      <td>19225.0</td>\n","      <td>1310.0</td>\n","      <td>120.0</td>\n","      <td>229.4</td>\n","    </tr>\n","    <tr>\n","      <th>175607</th>\n","      <td>PL002131</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>Med2</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>Biannual</td>\n","      <td>Yes</td>\n","      <td>WorkPrivate</td>\n","      <td>M</td>\n","      <td>58.0</td>\n","      <td>24.0</td>\n","      <td>Yes</td>\n","      <td>F</td>\n","      <td>23.0</td>\n","      <td>5.0</td>\n","      <td>demgvtbzilochupd</td>\n","      <td>9.0</td>\n","      <td>Gasoline</td>\n","      <td>Tourism</td>\n","      <td>160.0</td>\n","      <td>2088.0</td>\n","      <td>0.0</td>\n","      <td>270.0</td>\n","      <td>44.1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id_policy  year  ...  population town_surface_area\n","184715  PL076370   4.0  ...       440.0             224.8\n","80782   PL064491   2.0  ...       280.0             368.2\n","168786  PL077629   3.0  ...       120.0             229.4\n","175607  PL002131   4.0  ...       270.0              44.1\n","\n","[4 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"WoJEQhxMQtq9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611011340134,"user_tz":300,"elapsed":22536,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"4c9ff317-37f3-441f-fd40-0b7c01972ab6"},"source":["y_train.sample(n=4)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["72505     0.0\n","58523     0.0\n","115853    0.0\n","42906     0.0\n","Name: claim_amount, dtype: float64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ynDfq7F_d9Yh"},"source":["# Training the model üöÄ\n","\n","You must first define your first function: `fit_model`. This function takes training data as arguments, and outputs a \"model\" object -- that you define as you wish. For instance, this could be an array of parameter values."]},{"cell_type":"markdown","metadata":{"id":"CpW0yH_Lj2hG"},"source":["## Define your data preprocessing\n","\n","<a name=\"data-preprocessing\"></a>\n","\n","You can add any class or function in this cell for preprocessing. Just make sure that you use the functions here in the `fit_model`, `predict_expected_claim` and `predict_premium` functions if necessary. *italicised text*"]},{"cell_type":"code","metadata":{"id":"buq4-7IIjsUq","executionInfo":{"status":"ok","timestamp":1611011340135,"user_tz":300,"elapsed":22528,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["%%aicrowd_include\n","\n","# This magical command saves all code in this cell to a utils module.\n","# include your preprocessing functions and classes here.\n","\n","from sklearn.preprocessing import LabelBinarizer\n","\n","\n","class NormalizeData:\n","    '''\n","    Class used to normalize a dataset according to a standard normal \n","    distribution.\n","\n","    Methods\n","    -------\n","    fit : Use the training dataset to calculate the mean and standard deviation\n","        used for the normalisation of new data.\n","\n","    transform : Use the parameters calculated in the fit method to normalize \n","        new data.\n","    '''\n","    def __init__(self):\n","        self.x_means = 0\n","        self.x_std = 0\n","\n","    def fit(self, x_train):\n","        x_float = x_train.select_dtypes(include=['float', 'int']).drop(\n","            columns=['year', 'pol_no_claims_discount'])\n","        self.x_means = x_float.mean()\n","        self.x_std = x_float.std()\n","        return self\n","\n","    def transform(self, x_raw):\n","        for idx in x_raw:\n","            if idx in self.x_means.index:\n","                x_raw[idx] = (x_raw[idx] - self.x_means[idx]) / self.x_std[idx]\n","        return x_raw\n","\n","\n","class Compress_vh_make_model:\n","    '''\n","    Class used to group the labels with low frequency from the feature \n","    vh_make_model.\n","\n","    Methods\n","    -------\n","    fit : Use the training dataset to calculate the mean and standard deviation\n","        used for the normalisation of new data.\n","\n","    transform : Use the parameters calculated in the fit method to normalize \n","        new data.\n","    '''\n","    def __init__(self, n_occurences=30):\n","        self.n_occ = n_occurences\n","\n","    def fit(self, x_train):\n","        self.models_counts = x_train.vh_make_model.value_counts()\n","        self.models_to_group = self.models_counts[\n","                self.models_counts < self.n_occ].keys()\n","        return self\n","    \n","    def transform(self, x_raw):\n","        # Add a new feature according to the model count\n","        df_counts = pd.DataFrame(\n","            list(zip(self.models_counts.index, self.models_counts)),\n","            columns =['vh_make_model', 'vh_model_counts']\n","            )\n","        x_raw = x_raw.merge(\n","            right = df_counts, \n","            on = 'vh_make_model', \n","            how ='left'\n","            )\n","        x_raw['vh_model_counts'] = x_raw['vh_model_counts'].fillna(value=1)\n","\n","        # Compressing the column vh_make_model by grouping low frequency's models.\n","        mask_model_to_group = x_raw.vh_make_model.isin(self.models_to_group)\n","        x_raw.loc[mask_model_to_group, 'vh_make_model'] = 'other_models'\n","\n","        return x_raw\n","\n","\n","# class ImputeVhInformations:\n","#     '''\n","#     Imputation class for missing values\n","#     '''\n","#     def __init__(self):\n","#         self.speed_mean = 0\n","#         self.weight_mean = 0\n","#         self.age_mean = 0\n","\n","#     def prefit(self, x_train):\n","#         self.speed_mean = x_train['vh_speed'].mean()\n","#         self.weight_mean = x_train['vh_weight'].mean()\n","#         self.age_mean = x_train['vh_age'].mean()\n","#         return self\n","\n","#     def pretransform(self, x_raw):\n","#         x = x_raw.copy()\n","#         x.loc[x.vh_speed.isnull(), 'vh_speed'] = self.speed_mean\n","#         x.loc[x.vh_weight.isnull(), 'vh_weight'] = self.weight_mean\n","#         x.loc[x.vh_age.isnull(), 'vh_age'] = self.age_mean\n","#         return x\n","\n","#     def fit(self, x_train):\n","#         # Starting to impute speed and weight according to the model mean\n","#         self.prefit(x_train)\n","#         x = self.pretransform(x_train)\n","\n","#         # Training a mixed linear regression to impute the vh_value\n","#         vh_columns = ['vh_age', 'vh_fuel', 'vh_type']\n","#         x_imputation = x.loc[x.vh_value.notnull(), vh_columns]\n","#         le = LabelEncoder()\n","#         vh_make_model = le.fit_transform(x_train['vh_make_model'])\n","\n","#         self.vh_value_imputer = MixedLM(\n","#             endog = np.array(x['vh_value']),\n","#             exog = np.array(x_imputation),\n","#             groups = np.array(vh_make_model),\n","#             exog_re = np.array(x['vh_age'])\n","#             )\n","#         self.vh_value_imputer.fit()\n","\n","#         return self\n","\n","#     def transform(self, x_raw):\n","#         # Imputing speed and weight according to the model mean\n","#         x = self.pretransform(x_raw)\n","\n","#         # Predict the missing vh_values\n","#         vh_columns = ['vh_age', 'vh_fuel', 'vh_type']\n","#         rows_to_predict = x.vh_value.isnull()\n","\n","#         x_to_predict = x.loc[rows_to_predict, vh_columns]\n","#         x.loc[rows_to_predict, 'vh_value'] = self.vh_value_imputer.predict(x_to_predict)\n","\n","#         return x\n","\n","\n","class Preprocess_X_data:\n","    \"\"\"\n","    Class to preprocess the features of the dataset\n","\n","    Methods\n","    -------\n","    add_new_features : Method to include new features\n","\n","    impute_missing_values : Method to deal with missing values\n","\n","    fit : Use the training data set to specify the parameters of the \n","        prepocessing.\n","\n","    transform : Use the parameters from the fit method to preprocess new data.\n","\n","    \"\"\"\n","    def __init__(self, n_occurences_vh_make_model=30, drop_id=False):\n","        self.normalizer = NormalizeData()\n","        self.compress_models = Compress_vh_make_model(\n","            n_occurences=n_occurences_vh_make_model\n","            )\n","        self.cols_to_binarize = ['pol_payd', 'drv_sex1', 'drv_drv2']\n","        self.cols_to_one_hot_encode = [\n","            'pol_coverage', 'pol_usage', 'drv_sex2', \n","            'vh_make_model', 'vh_fuel', 'vh_type'\n","            ]\n","        self.drop_id = drop_id\n","\n","    def add_new_features(self, x_raw):\n","        x = x_raw.copy()\n","        # Adding new features\n","        x.insert(\n","            loc=len(x.columns),\n","            column='pop_density',\n","            value = x.population / x.town_surface_area\n","            )\n","        x.insert(\n","            loc=len(x.columns),\n","            column='vh_speed_drv_age_ratio',\n","            value = x.vh_speed / x.drv_age1\n","            )\n","        x.insert(\n","            loc=len(x.columns),\n","            column='potential_force_impact',\n","            value = x.vh_speed * x.vh_weight\n","            )\n","\n","        # Droping not necessay variables\n","        x = x.drop(columns='pol_pay_freq')\n","        if self.drop_id:\n","            x = x.drop(columns='id_policy')\n","        return x\n","\n","    def impute_missing_values(self, x_raw):\n","        x = x_raw.copy()\n","        # Adding missing indicators\n","        x['vh_age_NA'] = x['vh_age'].isnull()\n","        x['vh_value_NA'] = x['vh_value'].isnull()\n","\n","        # Impute missing values\n","        x = x.fillna(0)\n","        return x\n","\n","    def fit(self, x_train):\n","        # Adding new features\n","        x_train = self.add_new_features(x_train)\n","\n","        # Compressing the vh_make_model column\n","        self.compress_models.fit(x_train)\n","        x_train = self.compress_models.transform(x_train)\n","\n","        # Normalization\n","        self.normalizer.fit(x_train)\n","\n","        return self\n","\n","    def transform(self, x_raw):\n","        # Adding new features\n","        x_prep = self.add_new_features(x_raw)\n","\n","        # Compressing the vh_make_model column\n","        x_prep = self.compress_models.transform(x_prep)\n","\n","        # Normalization\n","        colnames = x_prep.columns\n","        x_prep = self.normalizer.transform(x_prep)\n","        x_prep = pd.DataFrame(x_prep, columns=colnames)\n","\n","        # Impute missing values\n","        x_prep = self.impute_missing_values(x_prep)\n","\n","        # Binarize columns with only two categories\n","        lb = LabelBinarizer()\n","        for col in self.cols_to_binarize:\n","            x_prep[col] = lb.fit_transform(x_prep[col])\n","\n","        # One-Hot-Encode the other categorical columns\n","        x_prep = pd.get_dummies(\n","            data=x_prep,\n","            prefix = self.cols_to_one_hot_encode,\n","            columns = self.cols_to_one_hot_encode,\n","            drop_first=True,\n","            dtype='int8'\n","            )\n","\n","        return x_prep\n","\n","    def fit_transform(self, x_raw):\n","        return self.fit(x_raw).transform(x_raw)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtL2L7SgFg0c","executionInfo":{"status":"ok","timestamp":1611011340135,"user_tz":300,"elapsed":22522,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["import importlib\n","import utils\n","importlib.reload(utils)\n","from utils import *  # do not change this"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAaNQuVxRTUs"},"source":["## Define the training logic"]},{"cell_type":"code","metadata":{"id":"Sq5eLv53lF3R","executionInfo":{"status":"ok","timestamp":1611011465690,"user_tz":300,"elapsed":326,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\r\n","\r\n","\r\n","def model_score(x_train, x_test, y_train, y_test, xgb_params):\r\n","    \"\"\"\r\n","    Function that calculate the evaluation score for the hyperparameters \r\n","    optimisation purpose.\r\n","    \"\"\"\r\n","    # Convert de features dataframes into DMatrix so it can be use to train an \r\n","    # XGBoost\r\n","    dmatrix_train = xgb.DMatrix(x_train)\r\n","    dmatrix_train.set_label(y_train)\r\n","    dmatrix_valid = xgb.DMatrix(x_test)\r\n","    dmatrix_valid.set_label(y_test)\r\n","\r\n","    # Train the XGBoost model\r\n","    results_dict = {}\r\n","    model = xgb.train(\r\n","        xgb_params,\r\n","        dtrain=dmatrix_train,\r\n","        num_boost_round=4000,\r\n","        early_stopping_rounds=50,\r\n","        evals=[(dmatrix_train, \"train\"), (dmatrix_valid, \"eval\")],\r\n","        evals_result=results_dict\r\n","    )\r\n","\r\n","    return float(list(results_dict[\"eval\"].values())[0][-1])\r\n","\r\n","\r\n","def hyperparameters_optimization(X_raw, y_raw):\r\n","\r\n","    xgb_params_space = {\r\n","        # Definition of the model to train\r\n","        \"objective\": \"reg:tweedie\",\r\n","        \"tweedie_variance_power\": hp.uniform(\"tweedie_variance_power\", 1, 2),\r\n","        \"booster\": 'gbtree',\r\n","        # Evaluation metric\r\n","        \"eval_metric\": \"rmse\",\r\n","        # Parameters for gbtree booster\r\n","        \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 0.2),\r\n","        \"n_estimators\": hp.choice(\"n_estimators\", range(1, 1000)),\r\n","        'gamma' : hp.uniform(\"gamma\", 0, 0.4),\r\n","        'lambda': hp.choice(\"lambda\", range(1, 1000)),\r\n","        \"alpha\": hp.choice(\"alpha\", range(1, 1000)),\r\n","        \"min_child_weight\": hp.choice(\"min_child_weight\", range(1, 12)),\r\n","        \"max_depth\": hp.choice(\"max_depth\", range(1, 10)),\r\n","        \"scale_pos_weight\": hp.choice(\"scale_pos_weight\", range(1, 1000)),\r\n","        'tree_method': 'gpu_hist',\r\n","        # Additionnal parameters for the training function\r\n","        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 0.9),\r\n","        \"subsample\": hp.uniform(\"subsample\", 0.5, 0.9),\r\n","    }\r\n","\r\n","    # Preprocessing\r\n","    print('preprocessing')\r\n","    preprocessing = Preprocess_X_data(\r\n","        n_occurences_vh_make_model=50,\r\n","        drop_id=True\r\n","        )\r\n","    x = preprocessing.fit_transform(X_raw)\r\n","    x = np.array(x)\r\n","    \r\n","    X_train, X_test, y_train, y_test = train_test_split(\r\n","        x, y_raw,\r\n","        test_size=0.33,\r\n","        shuffle=True,\r\n","        random_state=4000\r\n","    )\r\n","\r\n","\r\n","    def calculate_rmse(para):\r\n","        rmse = model_score(X_train, X_test, y_train, y_test, para)\r\n","        return {'loss': rmse, 'status': STATUS_OK}\r\n","\r\n","\r\n","    trials = Trials()\r\n","    print('Start')\r\n","    best = fmin(\r\n","        fn=calculate_rmse, \r\n","        space=xgb_params_space, \r\n","        algo=tpe.suggest, \r\n","        max_evals=100, \r\n","        trials=trials\r\n","        )\r\n","    print('best:')\r\n","    print(best)\r\n","    with open('xgb_best_params.json', 'w') as outfile:\r\n","        json.dump(best, outfile)\r\n","\r\n","\r\n","optim_parameters = False\r\n","if optim_parameters:\r\n","    hyperparameters_optimization(X_train, y_train)\r\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffOanSIvd9Yh","executionInfo":{"status":"ok","timestamp":1611011486721,"user_tz":300,"elapsed":332,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["def fit_model(X_raw, y_raw):\n","    \"\"\"Model training function: given training data (X_raw, y_raw), train this pricing model.\n","\n","    Parameters\n","    ----------\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","    y_raw : a Numpy array, with the value of the claims, in the same order as contracts in X_raw.\n","        A one dimensional array, with values either 0 (most entries) or >0.\n","\n","    Returns\n","    -------\n","    self: this instance of the fitted model. This can be anything, as long as it is compatible\n","        with your prediction methods.\n","\n","    \"\"\"    \n","    xgb_params = {\n","        # Definition of the model to train\n","        \"objective\": [\"reg:tweedie\"],\n","        \"tweedie_variance_power\" : [1.13],\n","        \"booster\" : ['gbtree'],\n","        # Evaluation metric\n","        \"eval_metric\": [\"rmse\"],\n","        # Parameters for gbtree booster\n","        'colsample_bytree': [0.85],\n","        'gamma': [0.2],\n","        'learning_rate': [0.18],\n","        'max_depth': [8],\n","        'min_child_weight': [2],\n","        'subsample': [0.6],\n","        'lambda':[1],\n","        'alpha':[0],\n","        'tree_method':['gpu_hist'],\n","        # Additionnal parameters for the training function\n","        \"early_stopping_rounds\": [25],\n","        \"num_boost_round\": [4000]\n","    }\n","\n","    # Preprocessing\n","    preprocessing = Preprocess_X_data(\n","        n_occurences_vh_make_model=50,\n","        drop_id=False\n","        )\n","    x = preprocessing.fit_transform(X_raw)\n","    x = x.drop(columns='id_policy', errors='ignore')\n","\n","    # Split the data in train and validation dataset according to the year\n","    x_train, x_valid, y_train, y_valid = train_test_split(\n","        x, y_raw, \n","        test_size=0.10,\n","        shuffle=True,\n","        random_state=2020\n","        )\n","    \n","    # Convert de features dataframes into DMatrix so it can be use to train an \n","    # XGBoost\n","    dmatrix_train = xgb.DMatrix(x_train.values)\n","    dmatrix_train.set_label(y_train)\n","    dmatrix_valid = xgb.DMatrix(x_valid.values)\n","    dmatrix_valid.set_label(y_valid)\n","\n","    # Transform xgb_params as a list of every combinations of parameters that \n","    # needs to be tried during the gridsearch.\n","    keys, values = zip(*xgb_params.items())\n","    param_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","    print(f\"Number of combinations of parameters to try: {len(param_list)}\")\n","\n","    # Train the XGBoost model\n","    results_list = list()\n","    for i, params_dict in enumerate(param_list):\n","        results_dict = {}\n","        model = xgb.train(\n","            params_dict,\n","            dtrain = dmatrix_train,\n","            num_boost_round = params_dict[\"num_boost_round\"],\n","            early_stopping_rounds = params_dict[\"early_stopping_rounds\"],\n","            evals = [(dmatrix_train, \"train\"), (dmatrix_valid, \"eval\")],\n","            evals_result = results_dict\n","        )\n","        results_list.append({\n","            \"eval\": float(list(results_dict[\"eval\"].values())[0][-1]),\n","            \"train\": float(list(results_dict[\"train\"].values())[0][-1]),\n","            \"params\": params_dict\n","        })\n","        print(f\"Trained model #{i + 1} out of {len(param_list)}\")\n","\n","    print(results_list)\n","\n","    return model, preprocessing\n"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CfjGFHcXd9Yh"},"source":["## Train your model"]},{"cell_type":"code","metadata":{"id":"0wtCLn_Xd9Yi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611011499654,"user_tz":300,"elapsed":11279,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"a5752f16-7a34-4928-80a9-863f98ae05cf"},"source":["trained_model = fit_model(X_train, y_train)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Number of combinations of parameters to try: 1\n","[0]\ttrain-rmse:754.638\teval-rmse:508.727\n","Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n","\n","Will train until eval-rmse hasn't improved in 25 rounds.\n","[1]\ttrain-rmse:753.953\teval-rmse:507.751\n","[2]\ttrain-rmse:752.373\teval-rmse:505.627\n","[3]\ttrain-rmse:749.997\teval-rmse:502.772\n","[4]\ttrain-rmse:747.374\teval-rmse:500.231\n","[5]\ttrain-rmse:744.363\teval-rmse:498.096\n","[6]\ttrain-rmse:740.17\teval-rmse:496.701\n","[7]\ttrain-rmse:730.127\teval-rmse:495.771\n","[8]\ttrain-rmse:718.546\teval-rmse:495.325\n","[9]\ttrain-rmse:702.631\teval-rmse:494.901\n","[10]\ttrain-rmse:696.813\teval-rmse:494.724\n","[11]\ttrain-rmse:675.76\teval-rmse:494.71\n","[12]\ttrain-rmse:661.48\teval-rmse:494.721\n","[13]\ttrain-rmse:650.406\teval-rmse:494.841\n","[14]\ttrain-rmse:637.966\teval-rmse:494.867\n","[15]\ttrain-rmse:619.508\teval-rmse:494.926\n","[16]\ttrain-rmse:603.739\teval-rmse:495.006\n","[17]\ttrain-rmse:585.9\teval-rmse:495.079\n","[18]\ttrain-rmse:573.147\teval-rmse:495.213\n","[19]\ttrain-rmse:564.477\teval-rmse:495.294\n","[20]\ttrain-rmse:559.556\teval-rmse:495.289\n","[21]\ttrain-rmse:556.744\teval-rmse:495.407\n","[22]\ttrain-rmse:551.95\teval-rmse:495.384\n","[23]\ttrain-rmse:553.759\teval-rmse:495.593\n","[24]\ttrain-rmse:550.195\teval-rmse:495.699\n","[25]\ttrain-rmse:541.292\teval-rmse:495.735\n","[26]\ttrain-rmse:540.488\teval-rmse:495.697\n","[27]\ttrain-rmse:539.294\teval-rmse:495.807\n","[28]\ttrain-rmse:535.599\teval-rmse:495.891\n","[29]\ttrain-rmse:529.039\teval-rmse:495.921\n","[30]\ttrain-rmse:526.335\teval-rmse:495.964\n","[31]\ttrain-rmse:525.223\teval-rmse:496.05\n","[32]\ttrain-rmse:523.859\teval-rmse:496.129\n","[33]\ttrain-rmse:524.972\teval-rmse:496.199\n","[34]\ttrain-rmse:524.141\teval-rmse:496.194\n","[35]\ttrain-rmse:523.497\teval-rmse:496.362\n","[36]\ttrain-rmse:522.011\teval-rmse:496.367\n","Stopping. Best iteration:\n","[11]\ttrain-rmse:675.76\teval-rmse:494.71\n","\n","Trained model #1 out of 1\n","[{'eval': 496.36203, 'train': 523.496521, 'params': {'objective': 'reg:tweedie', 'tweedie_variance_power': 1.13, 'booster': 'gbtree', 'eval_metric': 'rmse', 'colsample_bytree': 0.85, 'gamma': 0.2, 'learning_rate': 0.18, 'max_depth': 8, 'min_child_weight': 2, 'subsample': 0.6, 'lambda': 1, 'alpha': 0, 'tree_method': 'gpu_hist', 'early_stopping_rounds': 25, 'num_boost_round': 4000}}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NjUk7tfjd9Yi"},"source":["**Important note**: your training code should be able to run in under 10 minutes (since this notebook is re-run entirely on the server side). \n","\n","If you run into an issue here we recommend using the *zip file submission* (see the [challenge page](https://www.aicrowd.com/challenges/insurance-pricing-game/#how-to%20submit)). In short, you can simply do this by copy-pasting your `fit_model`, `predict_expected_claim` and `predict_premium` functions to the `model.py` file.\n","\n","Note that if you want to perform extensive cross-validation/hyper-parameter selection, it is better to do them offline, in a separate notebook."]},{"cell_type":"markdown","metadata":{"id":"LWYcr_Ued9Yi"},"source":["## Saving your model\n","\n","You can save your model to a file here, so you don't need to retrain it every time."]},{"cell_type":"code","metadata":{"id":"O6iWwkmHd9Yi","executionInfo":{"status":"ok","timestamp":1611011507434,"user_tz":300,"elapsed":361,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["def save_model(model_path):  # some models such xgboost models or keras models don't pickle very reliably. Please use the package provided saving functions instead. \n","  with open(model_path, 'wb') as target_file:\n","      pickle.dump(trained_model, target_file)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwEEP95EMow4","executionInfo":{"status":"ok","timestamp":1611011508450,"user_tz":300,"elapsed":334,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["save_model(Config.MODEL_OUTPUT_PATH)"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9G3KPnlsd9Yi"},"source":["If you need to load it from file, you can use this code:"]},{"cell_type":"code","metadata":{"id":"ICY88PT5d9Yi","executionInfo":{"status":"ok","timestamp":1611011509742,"user_tz":300,"elapsed":339,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["def load_model(model_path): # some models such xgboost models or keras models don't pickle very reliably. Please use the package provided saving functions instead. \n","  with open(model_path, 'rb') as target:\n","      return pickle.load(target)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxTX1TYOMsWK","executionInfo":{"status":"ok","timestamp":1611011510673,"user_tz":300,"elapsed":351,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["trained_model, preprocessing = load_model(Config.MODEL_OUTPUT_PATH)"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVeJiR1Ud9Yi"},"source":["# Predicting the claims üíµ\n","\n","The second function, `predict_expected_claim`, takes your trained model and a dataframe of contracts, and outputs a prediction for the (expected) claim incurred by each contract. This expected claim can be seen as the probability of an accident multiplied by the cost of that accident.\n","\n","This is the function used to compute the _RMSE_ leaderboard, where the model best able to predict claims wins."]},{"cell_type":"code","metadata":{"id":"rgM1xNf0d9Yi","executionInfo":{"status":"ok","timestamp":1611011513357,"user_tz":300,"elapsed":312,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}}},"source":["def predict_expected_claim(model, X_raw, preprocessing):\n","    \"\"\"Model prediction function: predicts the expected claim based on the pricing model.\n","\n","    This functions estimates the expected claim made by a contract (typically, as the product\n","    of the probability of having a claim multiplied by the expected cost of a claim if it occurs),\n","    for each contract in the dataset X_raw.\n","\n","    This is the function used in the RMSE leaderboard, and hence the output should be as close\n","    as possible to the expected cost of a contract.\n","\n","    Parameters\n","    ----------\n","    model: a Python object that describes your model. This can be anything, as long\n","        as it is consistent with what `fit` outpurs.\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","\n","    Returns\n","    -------\n","    avg_claims: a one-dimensional Numpy array of the same length as X_raw, with one\n","        expected claim per contract (in same order). These expected claims must be POSITIVE (>0).\n","    \"\"\"\n","    # Preprocessing\n","    x_clean = preprocessing.transform(X_raw)\n","    policy_id = x_clean.pop('id_policy')\n","    x_clean = xgb.DMatrix(x_clean.values)\n","\n","    # predictions\n","    expected_claim = model.predict(x_clean)\n","    expected_claim = pd.DataFrame({\n","        \"policy_id\": policy_id.values, \n","        \"expected_claim\": expected_claim\n","        })\n","\n","    return expected_claim"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FN7RqHcld9Yi"},"source":["To test your function, run it on your training data:"]},{"cell_type":"code","metadata":{"id":"P7Pu1UE-d9Yi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611015524162,"user_tz":300,"elapsed":8436,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"7daad458-dae3-4404-f72f-61b9041dd3ad"},"source":["expected_claims = predict_expected_claim(trained_model, X_train, preprocessing)\r\n","expected_claims['claim_amounts'] = y_train\r\n","print(expected_claims)"],"execution_count":117,"outputs":[{"output_type":"stream","text":["       policy_id  expected_claim  claim_amounts\n","0       PL000000       67.574654            0.0\n","1       PL042495       64.129890            0.0\n","2       PL042496      147.388367            0.0\n","3       PL042497       47.504368            0.0\n","4       PL042498       79.004623            0.0\n","...          ...             ...            ...\n","228211  PL008818      121.844688            0.0\n","228212  PL055033      117.793633            0.0\n","228213  PL061619      168.159470            0.0\n","228214  PL060903       27.699331            0.0\n","228215  PL052240      119.040894            0.0\n","\n","[228216 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8hqYUNEz2-2X"},"source":["# Leaderboard predictions\r\n","# Year 1\r\n","x = X_train[X_train.year == 1]\r\n","y = y_train[X_train.year == 1]\r\n","trained_model, preprocessing = fit_model(x, y)\r\n","expected_claims = predict_expected_claim(trained_model, x, preprocessing)\r\n","df_expectations = expected_claims.rename(columns={'expected_claim':'year 1'})\r\n","\r\n","# subsequent years\r\n","for year in [2,3,4]:\r\n","    x = X_train[X_train.year <= year]\r\n","    y = y_train[X_train.year <= year]\r\n","    trained_model, preprocessing = fit_model(x, y)\r\n","\r\n","    new_x = X_train[X_train.year == year]\r\n","    expected_claims = predict_expected_claim(trained_model, new_x, preprocessing)\r\n","    df_expectations = df_expectations.merge(\r\n","        expected_claims.rename(columns={'expected_claim':'year '+ str(year)}), \r\n","        on='policy_id', \r\n","        how='outer'\r\n","        )\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76bSBs721vb3","executionInfo":{"status":"ok","timestamp":1611015575283,"user_tz":300,"elapsed":345,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"1e9db60d-f092-4a4c-83e7-af3203368c26"},"source":["print(df_expectations)\r\n","sse = np.array([])\r\n","for i, col in enumerate(df_expectations.columns[1:]):\r\n","    y = np.array(y_train[X_train.year == i+1])\r\n","    y_predicted = np.array(df_expectations[col].values)\r\n","    sse = np.append(sse, (y_predicted - y)**2)\r\n","\r\n","sse.mean() ** 0.5"],"execution_count":118,"outputs":[{"output_type":"stream","text":["      policy_id      year 1      year 2      year 3      year 4\n","0      PL000000  138.475601  206.211472   53.509453   59.929661\n","1      PL042495   42.412300   63.934818   42.294338   46.201229\n","2      PL042496   31.804327   58.094189   29.981810   14.776361\n","3      PL042497   80.391930   78.044586   76.000435   57.384850\n","4      PL042498  115.126030   37.049774   30.784359   11.189279\n","...         ...         ...         ...         ...         ...\n","57049  PL002373  131.588135   86.257858   87.813972   98.504311\n","57050  PL004062    8.538408   46.636429   60.274456   30.683107\n","57051  PL006847  175.723526  126.055153  172.869980  105.745323\n","57052  PL012984   17.307497    5.385138    6.551529    2.292538\n","57053  PL008560   40.298351   40.566414   43.254398   33.405273\n","\n","[57054 rows x 5 columns]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["708.6987990366126"]},"metadata":{"tags":[]},"execution_count":118}]},{"cell_type":"markdown","metadata":{"id":"2LuitAiQd9Yi"},"source":["# Pricing contracts üí∞üí∞\n","\n","The third and final function, `predict_premium`, takes your trained model and a dataframe of contracts, and outputs a _price_ for each of these contracts. **You are free to set this prices however you want!** These prices will then be used in competition with other models: contracts will choose the model offering the lowest price, and this model will have to pay the cost if an accident occurs.\n","\n","This is the function used to compute the _profit_ leaderboard: your model will participate in many markets of size 10, populated by other participants' model, and we compute the average profit of your model over all the markets it participated in."]},{"cell_type":"code","metadata":{"id":"agmv13hnd9Yi"},"source":["def predict_premium(model, X_raw):\n","    \"\"\"Model prediction function: predicts premiums based on the pricing model.\n","\n","    This function outputs the prices that will be offered to the contracts in X_raw.\n","    premium will typically depend on the average claim predicted in \n","    predict_average_claim, and will add some pricing strategy on top.\n","\n","    This is the function used in the average profit leaderboard. Prices output here will\n","    be used in competition with other models, so feel free to use a pricing strategy.\n","\n","    Parameters\n","    ----------\n","    model: a Python object that describes your model. This can be anything, as long\n","        as it is consistent with what `fit` outpurs.\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","\n","    Returns\n","    -------\n","    prices: a one-dimensional Numpy array of the same length as X_raw, with one\n","        price per contract (in same order). These prices must be POSITIVE (>0).\n","    \"\"\"\n","\n","    # TODO: return a price for everyone.\n","    # Don't forget any preprocessing of the raw data here\n","\n","    return predict_expected_claim(model, X_raw) * 2  # Default: bosst prices by a factor of 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tu7T3lQ_d9Yi"},"source":["To test your function, run it on your training data."]},{"cell_type":"code","metadata":{"id":"P2Ej-1zcd9Yi"},"source":["prices = predict_premium(trained_model, X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcU5hWPHd9Yi"},"source":["#### Profit on training data\n","\n","In order for your model to be considered in the profit competition, it needs to make nonnegative profit over its training set. You can check that your model satisfies this condition below:"]},{"cell_type":"code","metadata":{"id":"hf389fhYd9Yi"},"source":["print('Income:', prices.sum())\n","print('Losses:', y_train.sum())\n","\n","if prices.sum() < y_train.sum():\n","    print('Your model loses money on the training data! It does not satisfy market rule 1: Non-negative training profit.')\n","    print('This model will be disqualified from the weekly profit leaderboard, but can be submitted for educational purposes to the RMSE leaderboard.')\n","else:\n","    print('Your model passes the non-negative training profit test!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQlsVqDqd9Yi"},"source":["# Ready? Submit to AIcrowd üöÄ\n","\n","If you are satisfied with your code, run the code below to send your code to the AICrowd servers for evaluation! This requires the variable `trained_model` to be defined by your previous code.\n","\n","**Make sure you have included all packages needed to run your code in the [_\"Packages\"_](#packages) section.**\n","\n","**NOTE**: If you submit the baseline RMSE model without any change whatsoever, your model will not be entered into the market. "]},{"cell_type":"code","metadata":{"id":"ovm0PyTEd9Yi"},"source":["%aicrowd_submit"],"execution_count":null,"outputs":[]}]}