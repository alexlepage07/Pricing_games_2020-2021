{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Submission_notebook_(Python) - XGB2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rWq4q0Gfp1Ha"},"source":["<div style=\"text-align: center\">\n","  <img alt=\"AIcrowd\" src=\"https://gitlab.aicrowd.com/jyotish/pricing-game-notebook-scripts/raw/master/pricing-game-banner.png\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"uIBNi6PiHFfD"},"source":["# How to use this notebook üìù\n","\n","1. **Copy the notebook**. This is a shared template and any edits you make here will not be saved. _You should copy it into your own drive folder._ For this, click the \"File\" menu (top-left), then \"Save a Copy in Drive\". You can edit your copy however you like.\n","2. **Link it to your AICrowd account**. In order to submit your code to AICrowd, you need to provide your account's API key (see [_\"Configure static variables\"_](#static-var) for details).\n","3. **Stick to the function definitions**. The submission to AICrowd will look for the pre-defined function names:\n","  - `fit_model`\n","  - `save_model`\n","  - `load_model`\n","  - `predict_expected_claim`\n","  - `predict_premium`\n","\n","    Anything else you write outside of these functions will not be part of the final submission (including constants and utility functions), so make sure everything is defined within them, except for:\n","4. **Define your preprocessing**. In addition to the functions above, anything in the cell labelled [_\"Define your data preprocessing\"_](#data-preprocessing) will also be imported into your final submission. "]},{"cell_type":"markdown","metadata":{"id":"uor1bk8ud9Yf"},"source":["# Your pricing model üïµÔ∏è\n","\n","In this notebook, you can play with the data, and define and train your pricing model. You can then directly submit it to the AICrowd, with some magic code at the end.\n","\n","### Baseline logistic regression üí™\n","You can also play with a baseline logistic regression model [implemented here](https://colab.research.google.com/drive/1iDgDgWUw9QzOkbTYjeyY3i3DGuCoghs3?usp=sharing). "]},{"cell_type":"markdown","metadata":{"id":"KOG9aspEPfLo"},"source":["# Setup the notebook üõ†"]},{"cell_type":"code","metadata":{"id":"Cc9aD_S9w_Qs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610658560306,"user_tz":300,"elapsed":9562,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"f1aeb2ea-b976-46fa-eda4-16092de315d8"},"source":["!bash <(curl -sL https://gitlab.aicrowd.com/jyotish/pricing-game-notebook-scripts/raw/master/python/setup.sh)\n","from aicrowd_helpers import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["‚öôÔ∏è Installing AIcrowd utilities...\n","  Running command git clone -q https://gitlab.aicrowd.com/yoogottamk/aicrowd-cli /tmp/pip-req-build-z3voe7_5\n","‚úÖ Installed AIcrowd utilities\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oWAkvr2mPqhO"},"source":["# Configure static variables üìé\n","<a name=\"static-var\"></a>\n","\n","In order to submit using this notebook, you must visit this URL https://aicrowd.com/participants/me and copy your API key. \n","\n","Then you must set the value of `AICROWD_API_KEY` wuth the value."]},{"cell_type":"code","metadata":{"id":"3Z8nmleFd9Yf"},"source":["import sklearn\n","\n","class Config:\n","  TRAINING_DATA_PATH = 'training.csv'\n","  MODEL_OUTPUT_PATH = 'model.pkl'\n","  AICROWD_API_KEY = 'eaab81e0ad4d64a6b0e7ec99a89205f6'  # You can get the key from https://aicrowd.com/participants/me\n","  ADDITIONAL_PACKAGES = [\n","    'numpy',  # you can define versions as well, numpy==0.19.2\n","    'pandas',\n","    'scikit-learn==' + sklearn.__version__,\n","    \"tqdm\",\n","    \"xgboost\"\n","    \"\"\n","  ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iK8Ki2WUjVoX"},"source":["# Download dataset files üíæ"]},{"cell_type":"code","metadata":{"id":"cgKzpAV0jVFQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610658580640,"user_tz":300,"elapsed":11895,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"951ecd57-82b2-4b8b-9006-79abf9c738aa"},"source":["# Make sure to offically join the challenge and accept the challenge rules! Otherwise you will not be able to download the data\n","%download_aicrowd_dataset"],"execution_count":null,"outputs":[{"output_type":"stream","text":["üíæ Downloading dataset...\n","Verifying API Key...\n","API Key valid\n","Saved API Key successfully!\n","‚úÖ Downloaded dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5wrBpC0qd9Yg"},"source":["# Packages üóÉ\n","\n","<a name=\"packages\"></a>\n","\n","Import here all the packages you need to define your model. **You will need to include all of these packages in `Config.ADDITIONAL_PACKAGES` for your code to run properly once submitted.**"]},{"cell_type":"code","metadata":{"id":"4q4C50Fsd9Yg"},"source":["%%track_imports\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from tqdm import tqdm\n","import xgboost as xgb\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR62QOUGd9Yg"},"source":["import importlib\n","import global_imports\n","importlib.reload(global_imports)\n","from global_imports import *  # do not change this"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRgsbwWwd9Yg"},"source":["# Loading the data üì≤"]},{"cell_type":"code","metadata":{"id":"vQQghMU7d9Yg"},"source":["df = pd.read_csv(Config.TRAINING_DATA_PATH)\n","X_train = df.drop(columns=['claim_amount'])\n","y_train = df['claim_amount']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WArx8uDQd9Yh"},"source":["## How does the data look like? üîç"]},{"cell_type":"code","metadata":{"id":"O_dyebPyQbSO","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1610658590352,"user_tz":300,"elapsed":543,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"a918cf5f-f29f-4c97-be14-52ab0c6d8aca"},"source":["X_train.sample(n=4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_policy</th>\n","      <th>year</th>\n","      <th>pol_no_claims_discount</th>\n","      <th>pol_coverage</th>\n","      <th>pol_duration</th>\n","      <th>pol_sit_duration</th>\n","      <th>pol_pay_freq</th>\n","      <th>pol_payd</th>\n","      <th>pol_usage</th>\n","      <th>drv_sex1</th>\n","      <th>drv_age1</th>\n","      <th>drv_age_lic1</th>\n","      <th>drv_drv2</th>\n","      <th>drv_sex2</th>\n","      <th>drv_age2</th>\n","      <th>drv_age_lic2</th>\n","      <th>vh_make_model</th>\n","      <th>vh_age</th>\n","      <th>vh_fuel</th>\n","      <th>vh_type</th>\n","      <th>vh_speed</th>\n","      <th>vh_value</th>\n","      <th>vh_weight</th>\n","      <th>population</th>\n","      <th>town_surface_area</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>42222</th>\n","      <td>PL072706</td>\n","      <td>1.0</td>\n","      <td>0.166</td>\n","      <td>Max</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>Yearly</td>\n","      <td>No</td>\n","      <td>WorkPrivate</td>\n","      <td>M</td>\n","      <td>29.0</td>\n","      <td>11.0</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>kpciudedjlrqsfte</td>\n","      <td>5.0</td>\n","      <td>Gasoline</td>\n","      <td>Tourism</td>\n","      <td>170.0</td>\n","      <td>13473.0</td>\n","      <td>980.0</td>\n","      <td>2530.0</td>\n","      <td>241.1</td>\n","    </tr>\n","    <tr>\n","      <th>16432</th>\n","      <td>PL020049</td>\n","      <td>1.0</td>\n","      <td>0.000</td>\n","      <td>Max</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>Yearly</td>\n","      <td>No</td>\n","      <td>WorkPrivate</td>\n","      <td>F</td>\n","      <td>53.0</td>\n","      <td>35.0</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>quslbttvcitxzeiy</td>\n","      <td>7.0</td>\n","      <td>Gasoline</td>\n","      <td>Tourism</td>\n","      <td>200.0</td>\n","      <td>19225.0</td>\n","      <td>1310.0</td>\n","      <td>50.0</td>\n","      <td>278.9</td>\n","    </tr>\n","    <tr>\n","      <th>48635</th>\n","      <td>PL001773</td>\n","      <td>1.0</td>\n","      <td>0.000</td>\n","      <td>Max</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>Biannual</td>\n","      <td>No</td>\n","      <td>Retired</td>\n","      <td>M</td>\n","      <td>69.0</td>\n","      <td>50.0</td>\n","      <td>Yes</td>\n","      <td>F</td>\n","      <td>55.0</td>\n","      <td>37.0</td>\n","      <td>lqkdgbosdzrtitgx</td>\n","      <td>5.0</td>\n","      <td>Diesel</td>\n","      <td>Tourism</td>\n","      <td>200.0</td>\n","      <td>22067.0</td>\n","      <td>1291.0</td>\n","      <td>100.0</td>\n","      <td>176.6</td>\n","    </tr>\n","    <tr>\n","      <th>74688</th>\n","      <td>PL068982</td>\n","      <td>2.0</td>\n","      <td>0.000</td>\n","      <td>Min</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>Biannual</td>\n","      <td>No</td>\n","      <td>WorkPrivate</td>\n","      <td>M</td>\n","      <td>57.0</td>\n","      <td>36.0</td>\n","      <td>No</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>cgkclpnidlmetsrb</td>\n","      <td>22.0</td>\n","      <td>Gasoline</td>\n","      <td>Tourism</td>\n","      <td>170.0</td>\n","      <td>18750.0</td>\n","      <td>900.0</td>\n","      <td>340.0</td>\n","      <td>113.8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id_policy  year  ...  population town_surface_area\n","42222  PL072706   1.0  ...      2530.0             241.1\n","16432  PL020049   1.0  ...        50.0             278.9\n","48635  PL001773   1.0  ...       100.0             176.6\n","74688  PL068982   2.0  ...       340.0             113.8\n","\n","[4 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"WoJEQhxMQtq9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610658591727,"user_tz":300,"elapsed":315,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"0b6d089b-442b-4114-8e93-ae2fbc919aa6"},"source":["y_train.sample(n=4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["191015       0.00\n","83491        0.00\n","220582       0.00\n","68450     4275.02\n","Name: claim_amount, dtype: float64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ynDfq7F_d9Yh"},"source":["# Training the model üöÄ\n","\n","You must first define your first function: `fit_model`. This function takes training data as arguments, and outputs a \"model\" object -- that you define as you wish. For instance, this could be an array of parameter values."]},{"cell_type":"markdown","metadata":{"id":"CpW0yH_Lj2hG"},"source":["## Define your data preprocessing\n","\n","<a name=\"data-preprocessing\"></a>\n","\n","You can add any class or function in this cell for preprocessing. Just make sure that you use the functions here in the `fit_model`, `predict_expected_claim` and `predict_premium` functions if necessary. *italicised text*"]},{"cell_type":"code","metadata":{"id":"buq4-7IIjsUq"},"source":["%%aicrowd_include\n","\n","# This magical command saves all code in this cell to a utils module.\n","# include your preprocessing functions and classes here.\n","\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelBinarizer\n","\n","\n","class NormalizeData:\n","    '''\n","    Class used to normalize a dataset according to a standard normal \n","    distribution.\n","\n","    Methods\n","    -------\n","    fit : Use the training dataset to calculate the mean and standard deviation\n","        used for the normalisation of new data.\n","\n","    transform : Use the parameters calculated in the fit method to normalize \n","        new data.\n","    '''\n","    def __init__(self):\n","        self.x_means = 0\n","        self.x_std = 0\n","\n","    def fit(self, x_train):\n","        x_float = x_train.select_dtypes(include=['float', 'int']).drop(\n","            columns=['year', 'pol_no_claims_discount'])\n","        self.x_means = x_float.mean()\n","        self.x_std = x_float.std()\n","        return self\n","\n","    def transform(self, x_raw):\n","        for idx in x_raw:\n","            if idx in self.x_means.index:\n","                x_raw[idx] = (x_raw[idx] - self.x_means[idx]) / self.x_std[idx]\n","        return x_raw\n","\n","\n","class Compress_vh_make_model:\n","    '''\n","    Class used to group the labels with low frequency from the feature \n","    vh_make_model.\n","\n","    Methods\n","    -------\n","    fit : Use the training dataset to calculate the mean and standard deviation\n","        used for the normalisation of new data.\n","\n","    transform : Use the parameters calculated in the fit method to normalize \n","        new data.\n","    '''\n","    def __init__(self, n_occurences=30):\n","        self.n_occ = n_occurences\n","\n","    def fit(self, x_train):\n","        models_counts = x_train.vh_make_model.value_counts()\n","        self.models_to_group = models_counts[models_counts < self.n_occ].keys()\n","        return self\n","    \n","    def transform(self, x_raw):\n","        mask_model_to_group = x_raw.vh_make_model.isin(self.models_to_group)\n","        x_raw.loc[mask_model_to_group, 'vh_make_model'] = 'other_models'\n","        x_raw['rare_models'] = mask_model_to_group\n","        return x_raw\n","\n","\n","class Preprocess_X_data:\n","    \"\"\"\n","    Class to preprocess the features of the dataset\n","\n","    Methods\n","    -------\n","    add_new_features : Method to include new features\n","\n","    impute_missing_values : Method to deal with missing values\n","\n","    fit : Use the training data set to specify the parameters of the \n","        prepocessing.\n","\n","    transform : Use the parameters from the fit method to preprocess new data.\n","\n","    \"\"\"\n","    def __init__(self, n_occurences_vh_make_model=30, drop_id=False):\n","        self.normalizer = NormalizeData()\n","        self.compress_models = Compress_vh_make_model(\n","            n_occurences=n_occurences_vh_make_model\n","            )\n","        self.cols_to_binarize = ['pol_payd', 'drv_sex1', 'drv_drv2']\n","        self.cols_to_one_hot_encode = [\n","            'pol_coverage', 'pol_usage', 'drv_sex2', \n","            'vh_make_model', 'vh_fuel', 'vh_type'\n","            ]\n","        self.drop_id = drop_id\n","\n","    def add_new_features(self, x_raw):\n","        x = x_raw.copy()\n","        # Adding new features\n","        x.insert(\n","            loc=len(x.columns),\n","            column='pop_density',\n","            value = x.population / x.town_surface_area\n","            )\n","        x.insert(\n","            loc=len(x.columns),\n","            column='vh_speed_drv_age_ratio',\n","            value = x.vh_speed / x.drv_age1\n","            )\n","        x.insert(\n","            loc=len(x.columns),\n","            column='potential_force_impact',\n","            value = x.vh_speed * x.vh_weight\n","            )\n","\n","        # Droping not necessay variables\n","        x = x.drop(columns='pol_pay_freq')\n","        if self.drop_id:\n","            x = x.drop(columns='id_policy')\n","        return x\n","\n","    def impute_missing_values(self, x_raw):\n","        x = x_raw.copy()\n","        # Adding missing indicators\n","        x['vh_age_NA'] = x['vh_age'].isnull()\n","        x['vh_value_NA'] = x['vh_value'].isnull()\n","\n","        # Impute missing values\n","        x = x.fillna(0)\n","        return x\n","\n","    def fit(self, x_train):\n","        # Adding new features\n","        x_train = self.add_new_features(x_train)\n","\n","        # Compressing the vh_make_model column\n","        self.compress_models.fit(x_train)\n","        x_train = self.compress_models.transform(x_train)\n","\n","        # Normalization\n","        self.normalizer.fit(x_train)\n","\n","        return self\n","\n","    def transform(self, x_raw):\n","        # Adding new features\n","        x_prep = self.add_new_features(x_raw)\n","\n","        # Compressing the vh_make_model column\n","        x_prep = self.compress_models.transform(x_prep)\n","\n","        # Normalization\n","        colnames = x_prep.columns\n","        x_prep = self.normalizer.transform(x_prep)\n","        x_prep = pd.DataFrame(x_prep, columns=colnames)\n","\n","        # Impute missing values\n","        x_prep = self.impute_missing_values(x_prep)\n","\n","        # Binarize columns with only two categories\n","        lb = LabelBinarizer()\n","        for col in self.cols_to_binarize:\n","            x_prep[col] = lb.fit_transform(x_prep[col])\n","\n","        # One-Hot-Encode the other categorical columns\n","        x_prep = pd.get_dummies(\n","            data=x_prep,\n","            prefix = self.cols_to_one_hot_encode,\n","            columns = self.cols_to_one_hot_encode,\n","            drop_first=True,\n","            dtype='int8'\n","            )\n","\n","        return x_prep\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtL2L7SgFg0c"},"source":["import importlib\n","import utils\n","importlib.reload(utils)\n","from utils import *  # do not change this"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAaNQuVxRTUs"},"source":["## Define the training logic"]},{"cell_type":"code","metadata":{"id":"ffOanSIvd9Yh"},"source":["def fit_model(X_raw, y_raw):\n","    \"\"\"Model training function: given training data (X_raw, y_raw), train this pricing model.\n","\n","    Parameters\n","    ----------\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","    y_raw : a Numpy array, with the value of the claims, in the same order as contracts in X_raw.\n","        A one dimensional array, with values either 0 (most entries) or >0.\n","\n","    Returns\n","    -------\n","    self: this instance of the fitted model. This can be anything, as long as it is compatible\n","        with your prediction methods.\n","\n","    \"\"\"\n","    TRAINING_YEARS = [1,2,3]\n","\n","    xgb_occ_params = {\n","        # Definition of the model to train\n","        \"objective\": [\"binary:logistic\"],\n","        \"booster\" : ['gbtree'],\n","        # Evaluation metric\n","        \"eval_metric\": [\"auc\"],\n","        # Parameters for gbtree booster\n","        'lambda':[1],\n","        \"alpha\": [1],\n","        'scale_pos_weight' : [5],\n","        'max_delta_step':[5],\n","        'tree_method':['gpu_hist'],\n","        # Additionnal parameters for the training function\n","        \"early_stopping_rounds\": [25],\n","        \"num_boost_round\": [4000]\n","    }\n","\n","    xgb_sev_params = {\n","        # Definition of the model to train\n","        \"objective\": [\"reg:tweedie\"],\n","        \"tweedie_variance_power\" : [1.8],\n","        \"booster\" : ['gbtree'],\n","        # Evaluation metric\n","        \"eval_metric\": [\"rmse\"],\n","        # Parameters for gbtree booster\n","        'lambda':[1],\n","        \"alpha\": [1],\n","        'tree_method':['gpu_hist'],\n","        # Additionnal parameters for the training function\n","        \"early_stopping_rounds\": [25],\n","        \"num_boost_round\": [4000]\n","    }\n","\n","    preprocessing = Preprocess_X_data(n_occurences_vh_make_model=50,\n","                                      drop_id=True)\n","    \n","    # Split the data in train and validation dataset according to the year\n","    x = X_raw.copy()\n","\n","    train_mask = x.year.isin(TRAINING_YEARS).to_list()\n","    valid_mask = (~x.year.isin(TRAINING_YEARS)).to_list()\n","\n","    x_train = x.loc[train_mask, :]\n","    y_train = y_raw[train_mask]\n","    x_valid = x.loc[valid_mask, :]\n","    y_valid = y_raw[valid_mask]\n","\n","    # Preprocessing\n","    preprocessing.fit(x_train)\n","    x_train = preprocessing.transform(x_train)\n","    x_valid = preprocessing.transform(x_valid)\n","\n","    # No more use of the column year\n","    x_train = x_train.drop(columns='year')\n","    x_valid = x_valid.drop(columns='year')\n","\n","\n","    def data_to_dmatrix(x_train, x_valid, y_train, y_valid):\n","        # Convert de features dataframes into DMatrix so it can be use to train an \n","        # XGBoost\n","        dmatrix_train = xgb.DMatrix(x_train.values)\n","        dmatrix_train.set_label(y_train)\n","        dmatrix_valid = xgb.DMatrix(x_valid.values)\n","        dmatrix_valid.set_label(y_valid)\n","        return dmatrix_train, dmatrix_valid\n","\n","\n","    def params_dict_to_list(param_dict):\n","        # Transform xgb_params as a list of every combinations of parameters that \n","        # needs to be tried during the gridsearch.\n","        keys, values = zip(*param_dict.items())\n","        param_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","        print(f\"Number of combinations of parameters to try: {len(param_list)}\")\n","        return param_list\n","\n","\n","    def train_xgb(dmatrix_train, dmatrix_valid, param_list):\n","        # Train the XGBoost model\n","        results_list = list()\n","        for i, params_dict in enumerate(param_list):\n","            results_dict = {}\n","            model = xgb.train(\n","                params_dict,\n","                dtrain = dmatrix_train,\n","                num_boost_round = params_dict[\"num_boost_round\"],\n","                early_stopping_rounds = params_dict[\"early_stopping_rounds\"],\n","                evals = [(dmatrix_train, \"train\"), (dmatrix_valid, \"eval\")],\n","                evals_result = results_dict\n","            )\n","            results_list.append({\n","                \"eval\": float(list(results_dict[\"eval\"].values())[0][-1]),\n","                \"train\": float(list(results_dict[\"train\"].values())[0][-1]),\n","                \"params\": params_dict\n","            })\n","            print(f\"Trained model #{i + 1} out of {len(param_list)}\")\n","        print(results_list)\n","        return model\n","    \n","    # fit occurence model\n","    y_train_occ = y_train > 0\n","    y_valid_occ = y_valid > 0\n","    dmatrix_train, dmatrix_valid = data_to_dmatrix(\n","        x_train, x_valid, y_train_occ, y_valid_occ)\n","    params_list = params_dict_to_list(xgb_occ_params)\n","    trained_model_occ = train_xgb(dmatrix_train, dmatrix_valid, params_list)\n","\n","    # fit severity model\n","    y_train_sev = y_train[y_train > 0]\n","    x_train_sev = x_train.loc[y_train > 0, :]\n","    y_valid_sev = y_valid[y_valid > 0]\n","    x_valid_sev = x_valid.loc[y_valid > 0, :]\n","    \n","    dmatrix_train, dmatrix_valid = data_to_dmatrix(\n","        x_train_sev, x_valid_sev, y_train_sev, y_valid_sev)\n","    params_list = params_dict_to_list(xgb_sev_params)\n","    trained_model_sev = train_xgb(dmatrix_train, dmatrix_valid, params_list)\n","\n","    trained_model = [trained_model_occ, trained_model_sev]\n","    return trained_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CfjGFHcXd9Yh"},"source":["## Train your model"]},{"cell_type":"code","metadata":{"id":"0wtCLn_Xd9Yi","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"error","timestamp":1610660110776,"user_tz":300,"elapsed":3518,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"ae276737-3e47-40f5-c1c0-11948dbe5d75"},"source":["trained_model = fit_model(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-75e320721ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-10e75bcf4e79>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(X_raw, y_raw)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# No more use of the column year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, x_raw)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols_to_binarize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mx_prep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_prep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# One-Hot-Encode the other categorical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    427\u001b[0m                              \"label binarization\")\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y has 0 samples: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: y has 0 samples: Series([], Name: pol_payd, dtype: object)"]}]},{"cell_type":"markdown","metadata":{"id":"NjUk7tfjd9Yi"},"source":["**Important note**: your training code should be able to run in under 10 minutes (since this notebook is re-run entirely on the server side). \n","\n","If you run into an issue here we recommend using the *zip file submission* (see the [challenge page](https://www.aicrowd.com/challenges/insurance-pricing-game/#how-to%20submit)). In short, you can simply do this by copy-pasting your `fit_model`, `predict_expected_claim` and `predict_premium` functions to the `model.py` file.\n","\n","Note that if you want to perform extensive cross-validation/hyper-parameter selection, it is better to do them offline, in a separate notebook."]},{"cell_type":"markdown","metadata":{"id":"LWYcr_Ued9Yi"},"source":["## Saving your model\n","\n","You can save your model to a file here, so you don't need to retrain it every time."]},{"cell_type":"code","metadata":{"id":"O6iWwkmHd9Yi"},"source":["def save_model(model_path):  # some models such xgboost models or keras models don't pickle very reliably. Please use the package provided saving functions instead. \n","  with open(model_path, 'wb') as target_file:\n","      pickle.dump(trained_model, target_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwEEP95EMow4"},"source":["save_model(Config.MODEL_OUTPUT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9G3KPnlsd9Yi"},"source":["If you need to load it from file, you can use this code:"]},{"cell_type":"code","metadata":{"id":"ICY88PT5d9Yi"},"source":["def load_model(model_path): # some models such xgboost models or keras models don't pickle very reliably. Please use the package provided saving functions instead. \n","  with open(model_path, 'rb') as target:\n","      return pickle.load(target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxTX1TYOMsWK"},"source":["trained_model = load_model(Config.MODEL_OUTPUT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVeJiR1Ud9Yi"},"source":["# Predicting the claims üíµ\n","\n","The second function, `predict_expected_claim`, takes your trained model and a dataframe of contracts, and outputs a prediction for the (expected) claim incurred by each contract. This expected claim can be seen as the probability of an accident multiplied by the cost of that accident.\n","\n","This is the function used to compute the _RMSE_ leaderboard, where the model best able to predict claims wins."]},{"cell_type":"code","metadata":{"id":"rgM1xNf0d9Yi"},"source":["def predict_expected_claim(model, X_raw):\n","    \"\"\"Model prediction function: predicts the expected claim based on the pricing model.\n","\n","    This functions estimates the expected claim made by a contract (typically, as the product\n","    of the probability of having a claim multiplied by the expected cost of a claim if it occurs),\n","    for each contract in the dataset X_raw.\n","\n","    This is the function used in the RMSE leaderboard, and hence the output should be as close\n","    as possible to the expected cost of a contract.\n","\n","    Parameters\n","    ----------\n","    model: a Python object that describes your model. This can be anything, as long\n","        as it is consistent with what `fit` outpurs.\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","\n","    Returns\n","    -------\n","    avg_claims: a one-dimensional Numpy array of the same length as X_raw, with one\n","        expected claim per contract (in same order). These expected claims must be POSITIVE (>0).\n","    \"\"\"\n","    \n","    # Preprocessing\n","    x = X_raw.copy()\n","    preprocessing = Preprocess_X_data(\n","        n_occurences_vh_make_model=50,\n","        drop_id=False\n","        )\n","    preprocessing.fit(X_train[X_train.year.isin([1,2,3])])\n","    x_clean = preprocessing.transform(x)\n","    x_clean = x_clean.drop(columns='year')\n","    policy_id = x_clean.pop('id_policy')\n","    x_clean = xgb.DMatrix(x_clean.values)\n","\n","    # predictions\n","    frequency_model, severity_model = model\n","\n","    expected_frequency = frequency_model.predict(x_clean)\n","    expected_severity = severity_model.predict(x_clean)\n","\n","    expected_claim = expected_frequency * expected_severity\n","\n","    return expected_claim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FN7RqHcld9Yi"},"source":["To test your function, run it on your training data:"]},{"cell_type":"code","metadata":{"id":"P7Pu1UE-d9Yi"},"source":["expected_claims = predict_expected_claim(trained_model, X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suvFyncv1TT0","executionInfo":{"status":"ok","timestamp":1610660370937,"user_tz":300,"elapsed":314,"user":{"displayName":"alexlepage007","photoUrl":"","userId":"16491687760320876052"}},"outputId":"b96b7033-b388-4ce2-8239-9233220ef040"},"source":["((expected_claims - y_train)**2).mean() ** 0.5"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["751.8928092363773"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"2LuitAiQd9Yi"},"source":["# Pricing contracts üí∞üí∞\n","\n","The third and final function, `predict_premium`, takes your trained model and a dataframe of contracts, and outputs a _price_ for each of these contracts. **You are free to set this prices however you want!** These prices will then be used in competition with other models: contracts will choose the model offering the lowest price, and this model will have to pay the cost if an accident occurs.\n","\n","This is the function used to compute the _profit_ leaderboard: your model will participate in many markets of size 10, populated by other participants' model, and we compute the average profit of your model over all the markets it participated in."]},{"cell_type":"code","metadata":{"id":"agmv13hnd9Yi"},"source":["def predict_premium(model, X_raw):\n","    \"\"\"Model prediction function: predicts premiums based on the pricing model.\n","\n","    This function outputs the prices that will be offered to the contracts in X_raw.\n","    premium will typically depend on the average claim predicted in \n","    predict_average_claim, and will add some pricing strategy on top.\n","\n","    This is the function used in the average profit leaderboard. Prices output here will\n","    be used in competition with other models, so feel free to use a pricing strategy.\n","\n","    Parameters\n","    ----------\n","    model: a Python object that describes your model. This can be anything, as long\n","        as it is consistent with what `fit` outpurs.\n","    X_raw : Pandas dataframe, with the columns described in the data dictionary.\n","        Each row is a different contract. This data has not been processed.\n","\n","    Returns\n","    -------\n","    prices: a one-dimensional Numpy array of the same length as X_raw, with one\n","        price per contract (in same order). These prices must be POSITIVE (>0).\n","    \"\"\"\n","\n","    # TODO: return a price for everyone.\n","    # Don't forget any preprocessing of the raw data here\n","\n","    return predict_expected_claim(model, X_raw) * 2  # Default: bosst prices by a factor of 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tu7T3lQ_d9Yi"},"source":["To test your function, run it on your training data."]},{"cell_type":"code","metadata":{"id":"P2Ej-1zcd9Yi"},"source":["prices = predict_premium(trained_model, X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcU5hWPHd9Yi"},"source":["#### Profit on training data\n","\n","In order for your model to be considered in the profit competition, it needs to make nonnegative profit over its training set. You can check that your model satisfies this condition below:"]},{"cell_type":"code","metadata":{"id":"hf389fhYd9Yi"},"source":["print('Income:', prices.sum())\n","print('Losses:', y_train.sum())\n","\n","if prices.sum() < y_train.sum():\n","    print('Your model loses money on the training data! It does not satisfy market rule 1: Non-negative training profit.')\n","    print('This model will be disqualified from the weekly profit leaderboard, but can be submitted for educational purposes to the RMSE leaderboard.')\n","else:\n","    print('Your model passes the non-negative training profit test!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQlsVqDqd9Yi"},"source":["# Ready? Submit to AIcrowd üöÄ\n","\n","If you are satisfied with your code, run the code below to send your code to the AICrowd servers for evaluation! This requires the variable `trained_model` to be defined by your previous code.\n","\n","**Make sure you have included all packages needed to run your code in the [_\"Packages\"_](#packages) section.**\n","\n","**NOTE**: If you submit the baseline RMSE model without any change whatsoever, your model will not be entered into the market. "]},{"cell_type":"code","metadata":{"id":"ovm0PyTEd9Yi"},"source":["%aicrowd_submit"],"execution_count":null,"outputs":[]}]}